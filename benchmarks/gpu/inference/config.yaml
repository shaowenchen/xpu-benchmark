# NVIDIA GPU Inference Configuration
hardware:
  type: "nvidia"
  driver_version: "auto"
  cuda_version: "auto"

inference:
  bert_tf_serving:
    enabled: true
    batch_size: 1
    sequence_length: 512
    model_path: "/models/bert-base-uncased"
    iterations: 1000

metrics:
  collection_interval: 1  # seconds
  gpu_metrics:
    - utilization
    - memory_used
    - memory_total
    - temperature
    - power_draw
  system_metrics:
    - cpu_usage
    - memory_usage

reporting:
  output_dir: "/app/reports"
  format: ["json", "csv"]
  generate_plots: true
  save_logs: true 